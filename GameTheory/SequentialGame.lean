
import GameTheory.StrategicGame

/-

A sequential game consisting of:
- a set of players
- a set of game states
- a set of possible actions (assumed the same for all players)
- a map 'turn' that tells whose turn it is from a given state
- a map 'move' that takes an action profile and updates the state
- each player has a preference on state trajectories

Some formalizations assume players take the state history as input for their decision.
For simplicity let's just assume the state variable already contains that information.

-/

-- The sequential game struct just comes with a mapping from action profiles to state updates.

variable {I E A: Type} [DecidableEq I]

class SeqGameStruct (I E A: Type) where
  move: (I → A) → E → E

-- This is enough to define the one-step update from an initial state along with the full state sequence.

def SeqGameStruct.step (G: SeqGameStruct I E A) (π: I → E → A) (ε: E): E :=
  G.move (fun p => π p ε) ε

def SeqGameStruct.run (G: SeqGameStruct I E A) (π: I → E → A) (ε: E): Nat → E :=
  fun n => match n with
  | Nat.zero => ε
  | Nat.succ prev => G.step π (G.run π ε prev)

-- A full sequential game comes with a preference relation on trajectories.

class SeqGame (I E A: Type) extends SeqGameStruct I E A where
  prefer: I → Relation (Nat → E)

-- Given a sequential game and an initial state, there is a corresponding outcome game where the outcomes are trajectories.

def SeqGame.toOutcomeGame (G: SeqGame I E A) (ε: E): OutcomeGame I (E → A) (Nat → E) := {
  play := fun π => G.run π ε
  prefer := G.prefer
}

-- A subgame perfect equilibrium is a strategy profile in which every subgame is a Nash equilibrium.

def subgame_perfect_equilibrium (G: SeqGame I E A) (π: I → E → A): Prop :=
  ∀ ε, nash_equilibrium (G.toOutcomeGame ε).toGame π

-- A sequential utility game in which every state comes with a utility for each players.
-- (This is simpler than assigning utilities to transitions, in principle I think you can pack these into the state.)

class SeqUtilityGame (I E A U: Type) extends SeqGameStruct I E A where
  uvalue: I → E → U
  prefer: Relation U

-- Next we will assume we have a map σ that "sums up" the utilities along trajectories.
-- In this case, the h-value (history value) of a trajectory is the sum of the sequence.
-- The π-value is the sum of the trajectory generated by π.

def hvalue (G: SeqUtilityGame I E A U) (σ: (Nat → U) → U) (h: Nat → E) (p: I): U :=
  σ (fun t => G.uvalue p (h t))

def πvalue (G: SeqUtilityGame I E A U) (σ: (Nat → U) → U) (π: I → E → A) (ε: E) (p: I): U :=
  let h := G.run π ε
  hvalue G σ h p

-- Given a sequential utility game, an initial state ε, and a utility summing function σ,
-- there is a corresponding utility game.

@[simp]
def SeqUtilityGame.toUtilityGame (G: SeqUtilityGame I E A U) (σ: (Nat → U) → U) (ε: E): UtilityGame I (E → A) U := {
  play := fun π =>
    let h := G.run π ε
    hvalue G σ h
  prefer := G.prefer
}



/-

Bellman optimality principle attempt:

Suppose we have a sequential utility game,
equipped with σ (sum-like) and α (addition-like)
such that for any sequence u0, u1, u2, ... of utilities:

σ(u0, u1, u2, ...) = α(u0, σ(u1, u2, ...))

(For standard discounted weighting take α(u, v) = u + βv, for nondiscounted take α(u, v) = u + v.)

Fix a state s, player p, let π0, π1 be profiles.
Let Up(s) be the utility player p associates to state s.
Let s0, s1 be the result of play under π0, π1 after one step starting from s.
Let Vp(s0, π0) and Vp(s1, π1) be the longterm value of states s0 and s1, played under π0 and π1 respectively.

Theorem: if Up(s) + Vp(s0, π0) ≤ Up(s) + Vp(s1, π0) then p prefers π0 ≤ π1 in the underlying normal-form game started from s.

-/

def tail {U: Type} (u: Nat → U): Nat → U :=
  fun t => u (t + 1)

def SeqUtilityGame.NormalForm (G: SeqUtilityGame I E A U) (σ: (Nat → U) → U) (ε: E): Game I (E → A) :=
  (G.toUtilityGame σ ε).toOutcomeGame.toGame

example (G: SeqUtilityGame I E A U)
  (σ: (Nat → U) → U)
  (α: U → U → U)
  (h0: ∀ u: Nat → U, α (u 0) (σ (tail u)) = σ u)
  (ε: E) (p: I) (π0 π1: I → E → A)
  (h1: G.prefer (α (G.uvalue p ε) (πvalue G σ π0 (G.move (flip π0 ε) ε) p)) (α (G.uvalue p ε) (πvalue G σ π1 (G.move (flip π1 ε) ε) p))):
  (G.NormalForm σ ε).prefer p π0 π1 := by
  simp_all [SeqUtilityGame.NormalForm, SeqUtilityGame.toUtilityGame, UtilityGame.toOutcomeGame, OutcomeGame.toGame]
  simp_all [πvalue, hvalue]
  rw [←h0]
  rw (config := {occs := .pos [2]}) [←h0]
  simp_all [SeqGameStruct.run]
  -- obnoxious lemma
  have {X Y: Type} {a b c d x: X} {f: X → X → Y} {g: Y → Y → Prop} (h1: g (f x a) (f x b)) (h2: a = c) (h3: b = d): g (f x c) (f x d) := by
    rw [←h2, ←h3]
    exact h1
  apply this h1 <;> (
    congr
    ext t
    simp [tail, SeqGameStruct.run]
    congr
    induction t with
    | zero => simp [SeqGameStruct.run]; rfl
    | succ previous ih => simp [SeqGameStruct.run, ih]
  )

-- TODO: definition of a valuation and something interesting about them?

-- Let G be a sequential game and fix a player p.
-- An arbitrary function v: E → U along with a preference on U is called a valuation for player p
-- if maximizing v always leads to preferable trajectories.

-- Note: this doesn't depend on the utility at ε itself?
def valuation (G: SeqGame I E A) (p: I) (Vp: E → U) (upref: Relation U): Prop :=
  ∀ π0 π1 ε, G.prefer p (G.run π0 ε) (G.run π1 ε) ↔ upref (Vp (G.step π0 ε)) (Vp (G.step π1 ε))
